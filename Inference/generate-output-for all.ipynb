{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from glob import glob\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import albumentations as A\n",
    "\n",
    "args = Namespace(\n",
    "    janus_model_path = 'deepseek-ai/Janus-Pro-1B',\n",
    "    model_path = '/home/rmuproject/rmuproject/users/sandesh/Depth-to-Image/Fine-tune-DreamBooth/outputs',\n",
    "    pretrained_model_path = 'stabilityai/stable-diffusion-2-depth',\n",
    "    image_path = '/home/rmuproject/rmuproject/users/sandesh/Depth-to-Image/Fine-tune-DreamBooth/Input',\n",
    "    input_path_list = [],\n",
    "    output_path = '/home/rmuproject/rmuproject/users/sandesh/Depth-to-Image/Fine-tune-DreamBooth/Output',\n",
    "    single_image_path = '/home/rmuproject/rmuproject/users/sandesh/Depth-to-Image/Fine-tune-DreamBooth/Input/input_7.png',\n",
    "    augment1 = A.Compose([\n",
    "        A.OneOf([\n",
    "            A.HorizontalFlip(p=0.9),  # Flip horizontally\n",
    "            A.VerticalFlip(p=0.9)], p=0.7)\n",
    "    ]),    # Flip vertically\n",
    "    augment2 = A.Compose([\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.5, p=0.8),  # Adjust brightness and contrast\n",
    "        A.CLAHE(clip_limit=2.0, tile_grid_size=(8,8), p=0.7),  # Apply Contrast Limited Adaptive Histogram Equalization\n",
    "        A.Equalize(p=0.6),  # Apply histogram equalization\n",
    "        A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.4), # Randomly change brightness, contrast, and saturation\n",
    "        A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.7),]),      # Adjust hue and saturation\n",
    "    output_alphabet = ['a', 'b', 'c', 'd', 'e'],\n",
    "    seed = [42, 123, 456, 789, 112],\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    prompt_for_understanding_image = 'Analyze the depth information of the image and describe its structural details, 4k quality, foreground and background separation, and intricate textures. Emphasize the key depth-based elements that define the scene and generate a detailed textual description suitable for guiding image variation synthesis in 2 sentences',\n",
    "    answer = [],\n",
    "    negative_prompt = \"bad, deformed, ugly, bad anotomy, bad resolution, bad quality, bad asthetic, blurry\",\n",
    "    num_variations = 5,\n",
    "    upscale_width = [],\n",
    "    upscale_height = [],\n",
    "    augmented_folder = '/home/rmuproject/rmuproject/users/sandesh/Depth-to-Image/Fine-tune-DreamBooth/Inference/5-variant_folder/augmented',\n",
    "    generated_folder = '/home/rmuproject/rmuproject/users/sandesh/Depth-to-Image/Fine-tune-DreamBooth/Inference/5-variant_folder/generated',\n",
    "    augmented_folder_list = [],\n",
    "    generated_folder_list = [],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inference_utils\n",
    "import torch\n",
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from IPython.display import display\n",
    "from urllib.request import urlopen\n",
    "\n",
    "config = AutoConfig.from_pretrained(args.janus_model_path)\n",
    "language_config = config.language_config\n",
    "language_config._attn_implementation = 'eager'\n",
    "vl_gpt = AutoModelForCausalLM.from_pretrained(args.janus_model_path,\n",
    "                                             language_config=language_config,\n",
    "                                             trust_remote_code=True)\n",
    "if args.device == 'cuda':\n",
    "    vl_gpt = vl_gpt.to(torch.bfloat16).cuda()\n",
    "else:\n",
    "    vl_gpt = vl_gpt.to(torch.float16)\n",
    "\n",
    "vl_chat_processor = VLChatProcessor.from_pretrained(args.janus_model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import requests\n",
    "from PIL import Image\n",
    "import random\n",
    "from diffusers import StableDiffusionDepth2ImgPipeline\n",
    "from diffusers.utils import load_image\n",
    "\n",
    "pipe1 = StableDiffusionDepth2ImgPipeline.from_pretrained(\n",
    "    args.model_path,\n",
    "    torch_dtype=torch.float16,\n",
    ").to(args.device)\n",
    "pipe1.enable_model_cpu_offload()\n",
    "\n",
    "import torch\n",
    "from diffusers import StableDiffusionUpscalePipeline\n",
    "from PIL import Image\n",
    "\n",
    "# Load the pre-trained upscaler model from Stability AI\n",
    "pipe2 = StableDiffusionUpscalePipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-x4-upscaler\", torch_dtype=torch.float16\n",
    ").to(\"cuda\")\n",
    "pipe2.enable_model_cpu_offload()\n",
    "\n",
    "import torch\n",
    "from diffusers import StableDiffusionXLImg2ImgPipeline\n",
    "from diffusers.utils import load_image\n",
    "\n",
    "pipe3 = StableDiffusionXLImg2ImgPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-refiner-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n",
    ")\n",
    "pipe3 = pipe3.to(\"cuda\")\n",
    "pipe3.enable_model_cpu_offload()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "args.input_path_list = sorted(glob(args.image_path + '/*'))\n",
    "len(args.input_path_list)\n",
    "args.input_path_list[:10]\n",
    "def sort_key(path):\n",
    "    filename = os.path.basename(path)\n",
    "    numbers = re.findall(r'\\d+', filename)\n",
    "    return int(numbers[0]) if numbers else float('inf')\n",
    "\n",
    "args.input_path_list = sorted(args.input_path_list, key=sort_key)\n",
    "args.input_path_list[:10], len(args.input_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(args.input_path_list)):\n",
    "    # print(args.input_path_list[i])\n",
    "    answers = []\n",
    "    args.upscale_width = [] \n",
    "    args.upscale_height = []\n",
    "    for j in range(args.num_variations):\n",
    "    # Load the image\n",
    "        init_image = Image.open(args.input_path_list[i]).convert(\"RGB\")\n",
    "        init_image_np = np.array(init_image)\n",
    "        \n",
    "        # Get the original image dimensions\n",
    "        input_width, input_height = init_image.size\n",
    "        args.upscale_width.append(input_width // 2)\n",
    "        args.upscale_height.append(input_height // 2)\n",
    "        # Compute new dimensions\n",
    "        new_width, new_height = input_width // 2, input_height // 2\n",
    "        \n",
    "        random.seed(args.seed[i] + j)  # Modify seed to ensure different variations for the same image\n",
    "        np.random.seed(args.seed[i] + j)\n",
    "        torch.manual_seed(args.seed[i] + j)\n",
    "\n",
    "        # Alternate between the two augmentation methods\n",
    "        if j % 2 == 0:\n",
    "            # First method (Augmentation 1 + Augmentation 2 + RandomCrop)\n",
    "            # Apply the first augmentation (flip transformations)\n",
    "            augmented_image = args.augment1(image=init_image_np)\n",
    "            init_image = Image.fromarray(augmented_image['image'])\n",
    "\n",
    "            # Apply the second set of augmentations (brightness, contrast, color)\n",
    "            augmented_image = args.augment2(image=np.array(init_image))\n",
    "            init_image = Image.fromarray(augmented_image['image'])\n",
    "            \n",
    "            # Apply random crop transformation\n",
    "            augmented_image = A.RandomCrop(width=args.upscale_width[j], height=args.upscale_height[i], p=1.0)\n",
    "            cropped_image = augmented_image(image=np.array(init_image))['image']\n",
    "\n",
    "            # Convert back to PIL Image\n",
    "            init_image2 = Image.fromarray(cropped_image)\n",
    "\n",
    "        else:\n",
    "            # Second method (Augmentation 1 + Augmentation 2 + Resize + Crop)\n",
    "            # Apply the first augmentation (flip transformations)\n",
    "            augmented_image1 = args.augment1(image=init_image_np)\n",
    "            init_image1 = Image.fromarray(augmented_image1['image'])\n",
    "\n",
    "            # Apply the second set of augmentations (brightness, contrast, color)\n",
    "            augmented_image2 = args.augment2(image=np.array(init_image1))\n",
    "            init_image2 = Image.fromarray(augmented_image2['image'])\n",
    "            \n",
    "            # Apply resize transformation\n",
    "            resize_transform = A.Resize(width=new_width, height=new_height, p=1.0)\n",
    "            init_image2 = resize_transform(image=np.array(init_image2))['image']  # Ensure this returns a dictionary and access 'image'\n",
    "            \n",
    "            # Apply random crop transformation\n",
    "            crop_transform = A.RandomCrop(width=args.upscale_width[i], height=args.upscale_height[i], p=1.0)\n",
    "            cropped_image = crop_transform(image=np.array(init_image2))['image']  # Same here, access 'image' in the dict\n",
    "\n",
    "            # Convert back to PIL Image\n",
    "            init_image2 = Image.fromarray(cropped_image)\n",
    "\n",
    "        # Optionally, save or display the final image\n",
    "        init_image2.save(f\"{args.augmented_folder}/augmented_image{i}{args.output_alphabet[j]}.png\")\n",
    "        \n",
    "    args.augmented_folder_list = glob(f\"{args.augmented_folder}/*.png\")\n",
    "    for j in range(args.num_variations):\n",
    "        answer = inference_utils.multimodal_understanding(\n",
    "            image = args.augmented_folder_list[i],\n",
    "            question=args.prompt_for_understanding_image,\n",
    "            seed = args.seed[i],\n",
    "            top_p = 0.8,\n",
    "            temperature = 1.0,\n",
    "            vl_chat_processor = vl_chat_processor,\n",
    "            vl_gpt = vl_gpt,\n",
    "            tokenizer = tokenizer,\n",
    "            cuda_device = args.device,\n",
    "        )\n",
    "        answers.append(answer)\n",
    "\n",
    "    for j in range(args.num_variations):\n",
    "        init_image2 = Image.open(args.augmented_folder_list[j]).convert(\"RGB\")\n",
    "        prompt = answers[j]\n",
    "        alphabet = args.output_alphabet[j]\n",
    "        seed = args.seed[j]\n",
    "        generator = torch.Generator(device=args.device).manual_seed(seed)\n",
    "        strength, guidance_scale = inference_utils.__generate_strength(seed), inference_utils.__generate_guidance_scale(seed)\n",
    "        # print(strength, guidance_scale)\n",
    "        image = pipe1(prompt=prompt, image=init_image2, negative_prompt=args.negative_prompt, strength=strength, guidance_scale=guidance_scale, generator=generator).images[0]\n",
    "        image.save(f\"{args.output_path}/output_{i+1}{args.output_alphabet[j]}.png\")\n",
    "        path = f\"{args.output_path}/output_{i+1}{args.output_alphabet[j]}.png\"\n",
    "        image = Image.open(path).convert(\"RGB\")\n",
    "\n",
    "        # Resize the image to half of the target resolution (if target is 2x upscaling)\n",
    "        width, height = image.size\n",
    "        image_resized = image.resize((width // 2, height // 2))\n",
    "        # print(f'Resized image dimensions: {image_resized.size}')\n",
    "        # Define your prompt\n",
    "        prompt = answers[j]\n",
    "        # Upscale image by 2x using the Stable Diffusion model\n",
    "        upscaled_image = pipe2(prompt=prompt, image=image_resized, guidance_scale=7.5, num_inference_steps = 30).images[0]\n",
    "        upscaled_image = upscaled_image.resize((input_width, input_height))\n",
    "        upscaled_image.save(f\"{args.output_path}/output_{i+1}{args.output_alphabet[j]}.png\")\n",
    "        img = cv2.imread(f\"{args.output_path}/output_{i+1}{args.output_alphabet[j]}.png\")\n",
    "        # Create a blurred version of the image\n",
    "        blurred = cv2.GaussianBlur(img, (5,5), 0)\n",
    "\n",
    "        # Apply unsharp masking\n",
    "        sharp = cv2.addWeighted(img, 1.5, blurred, -0.5, 0)\n",
    "\n",
    "        cv2.imwrite(f\"{args.output_path}/output_{i+1}{args.output_alphabet[j]}.png\", sharp)\n",
    "        img = cv2.imread(f\"{args.output_path}/output_{i+1}{args.output_alphabet[j]}.png\")\n",
    "        hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "        # Increase saturation\n",
    "        h, s, v = cv2.split(hsv)\n",
    "        s = cv2.add(s, 30)  # Adjust this value for more/less intensity\n",
    "        s = np.clip(s, 0, 255)\n",
    "\n",
    "        # Merge back and convert to BGR\n",
    "        enhanced_hsv = cv2.merge((h, s, v))\n",
    "        final = cv2.cvtColor(enhanced_hsv, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "        cv2.imwrite(f\"{args.output_path}/output_{i+1}{args.output_alphabet[j]}.png\", final)\n",
    "        \n",
    "        img = cv2.imread(f\"{args.output_path}/output_{i+1}{args.output_alphabet[j]}.png\")\n",
    "        inc = np.array([15, 10, -5], dtype=\"int16\")  # Adjust RGB values for warmth/coolness\n",
    "        balanced = np.clip(img + inc, 0, 255).astype(np.uint8)\n",
    "\n",
    "        cv2.imwrite(f\"{args.output_path}/output_{i+1}{args.output_alphabet[j]}.png\", balanced)\n",
    "        image = Image.open(f\"{args.output_path}/output_{i+1}{args.output_alphabet[j]}.png\").convert(\"RGB\")\n",
    "        prompt = answer[j]\n",
    "        image = pipe3(prompt= prompt, image=image, requires_aesthetics_score = True).images[0]\n",
    "        image.save(f\"{args.output_path}/output_{i+1}{args.output_alphabet[j]}.png\")\n",
    "    if i == 0:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(1, 6, figsize=(9, 12))\n",
    "init_image = Image.open(args.input_path_list[0]).convert(\"RGB\")\n",
    "ax[0].imshow(init_image)\n",
    "ax[0].set_title(\"Original Image\")\n",
    "ax[0].axis(\"off\")\n",
    "for j in range(args.num_variations):\n",
    "    variation_path = os.path.join(args.output_path, f\"output_1{args.output_alphabet[j]}.png\")\n",
    "    variation_image = Image.open(variation_path).convert(\"RGB\")\n",
    "\n",
    "    ax[j+1].imshow(variation_image)\n",
    "    ax[j+1].set_title(f\"Variation {j+1}\")\n",
    "    ax[j+1].axis(\"off\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
